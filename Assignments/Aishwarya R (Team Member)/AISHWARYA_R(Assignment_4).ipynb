{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKAyR-KillqR"
      },
      "source": [
        "> import pandas as pd\n",
        ">\n",
        "> spam=  \n",
        "> pd.read_csv('C:/Users/Admin/Desktop/Nalaiyathiran/assign/spam.csv',del\n",
        "> imiter=',',encoding='latin-1')\n",
        ">\n",
        "> spam.head()\n",
        ">\n",
        "> v1 v2 Unnamed: 2 \\\\  \n",
        "> 0 ham Go until jurong point, crazy.. Available only ... NaN\n",
        ">\n",
        "> 1 ham Ok lar... Joking wif u oni... NaN\n",
        ">\n",
        "> 2 spam Free entry in 2 a wkly comp to win FA Cup fina... NaN\n",
        ">\n",
        "> 3 ham U dun say so early hor... U c already then say... NaN\n",
        ">\n",
        "> 4 ham Nah I don't think he goes to usf, he lives aro... NaN\n",
        ">\n",
        "> Unnamed: 3 Unnamed: 4  \n",
        "> 0 NaN NaN  \n",
        "> 1 NaN NaN  \n",
        "> 2 NaN NaN  \n",
        "> 3 NaN NaN  \n",
        "> 4 NaN NaN\n",
        ">\n",
        "> spam.drop(\\['Unnamed: 2', 'Unnamed: 3', 'Unnamed:\n",
        "> 4'\\],axis=1,inplace=True)  \n",
        "> spam.info()\n",
        ">\n",
        "> \\<class 'pandas.core.frame.DataFrame'\\> RangeIndex: 5572 entries, 0 to\n",
        "> 5571 Data columns (total 2 columns):  \n",
        "> \\# Column Non-Null Count Dtype --- ------ -------------- ----- 0 v1\n",
        "> 5572 non-null object 1 v2 5572 non-null object dtypes: object(2)  \n",
        "> memory usage: 87.2+ KB\n",
        ">\n",
        "> import numpy as np  \n",
        "> import matplotlib.pyplot as plt  \n",
        "> import seaborn as sns  \n",
        "> from sklearn.preprocessing import LabelEncoder\n",
        ">\n",
        "> sns.countplot(spam.v1)  \n",
        "> plt.xlabel('Label')  \n",
        "> plt.title('Number of ham and spam messages') X = spam.v2\n",
        ">\n",
        "> Y = spam.v1  \n",
        "> le = LabelEncoder()  \n",
        "> Y = le.fit_transform(Y)  \n",
        "> Y = Y.reshape(-1,1)\n",
        ">\n",
        "> C:\\\\Users\\\\Admin\\\\anaconda3\\\\lib\\\\site-packages\\\\seaborn\\\\\\_decorators.py:36:\n",
        "> FutureWarning: Pass the following variable as a keyword arg: x. From\n",
        "> version 0.12, the only valid positional argument will be \\`data\\`, and\n",
        "> passing other arguments without an explicit keyword will result in an\n",
        "> error or misinterpretation.\n",
        ">\n",
        "> warnings.warn(\n",
        ">\n",
        "> <img src=\"attachment:vertopal_8143787163c442de8333cf0f618e5889/media/image1.png\" style=\"width:5.48472in;height:3.86111in\" />\n",
        ">\n",
        "> from sklearn.model_selection import train_test_split  \n",
        "> from sklearn.preprocessing import LabelEncoder  \n",
        "> from keras.models import Model  \n",
        "> from keras.layers import LSTM, Activation, Dense, Dropout, Input,\n",
        "> Embedding  \n",
        "> from keras.preprocessing.text import Tokenizer  \n",
        "> from keras.preprocessing import sequence  \n",
        "> from keras.callbacks import EarlyStopping  \n",
        "> %matplotlib inline\n",
        ">\n",
        "> X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.15)\n",
        ">\n",
        "> from keras.preprocessing.sequence import pad_sequences\n",
        ">\n",
        "> max_words =1000  \n",
        "> max_len =150  \n",
        "> tok = Tokenizer(num_words=max_words)\n",
        ">\n",
        "> tok.fit_on_texts(X_train)  \n",
        "> sequences = tok.texts_to_sequences(X_train)  \n",
        "> sequences_matrix = pad_sequences(sequences,maxlen=max_len)\n",
        ">\n",
        "> **def** RNN():  \n",
        "> inputs = Input(name='inputs',shape=\\[max_len\\])  \n",
        "> layer = Embedding(max_words,50,input_length=max_len)(inputs) layer =\n",
        "> LSTM(64)(layer)  \n",
        "> layer = Dense(256,name='FC1')(layer)  \n",
        "> layer = Activation('relu')(layer)  \n",
        "> layer = Dropout(0.5)(layer)  \n",
        "> layer = Dense(1,name='out_layer')(layer)  \n",
        "> layer = Activation('sigmoid')(layer)  \n",
        "> model = Model(inputs=inputs,outputs=layer)  \n",
        "> **return** model\n",
        ">\n",
        "> model = RNN()  \n",
        "> model.summary()\n",
        ">\n",
        "> Model: \"model\"  \n",
        "> \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n",
        "> Layer (type) Output Shape Param \\#\n",
        "> =================================================================\n",
        "> inputs (InputLayer) \\[(None, 150)\\] 0  \n",
        "> embedding (Embedding) (None, 150, 50) 50000 lstm (LSTM) (None, 64)\n",
        "> 29440 FC1 (Dense) (None, 256) 16640 activation (Activation) (None,\n",
        "> 256) 0  \n",
        "> dropout (Dropout) (None, 256) 0  \n",
        "> out_layer (Dense) (None, 1) 257 activation_1 (Activation) (None, 1)\n",
        "> 0  \n",
        "> =================================================================\n",
        "> Total params: 96,337  \n",
        "> Trainable params: 96,337  \n",
        "> Non-trainable params: 0  \n",
        "> \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n",
        ">\n",
        "> from tensorflow.keras.optimizers import RMSprop\n",
        ">\n",
        "> model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=\\[\n",
        "> 'accuracy'\\])\n",
        ">\n",
        "> model.fit(sequences_matrix,Y_train,batch_size=128,epochs=10,  \n",
        "> validation_split=0.2,callbacks=\\[EarlyStopping(monitor='val_loss',min_d\n",
        "> elta=0.0001)\\])\n",
        ">\n",
        "> Epoch 1/10  \n",
        "> 30/30 \\[==============================\\] - 6s 144ms/step - loss:\n",
        "> 0.3212 - accuracy: 0.8743 - val_loss: 0.1448 - val_accuracy: 0.9473  \n",
        "> Epoch 2/10  \n",
        "> 30/30 \\[==============================\\] - 4s 129ms/step - loss:\n",
        "> 0.0865 - accuracy: 0.9810 - val_loss: 0.0528 - val_accuracy: 0.9873\n",
        ">\n",
        "> \\<keras.callbacks.History at 0x207c54e3a90\\>\n",
        ">\n",
        "> model.save('Spam.h5')\n",
        ">\n",
        "> test_sequences = tok.texts_to_sequences(X_test)  \n",
        "> test_sequences_matrix = pad_sequences(test_sequences,maxlen=max_len)\n",
        "> test_sequences_matrix\n",
        ">\n",
        "> array(\\[\\[ 0, 0, 0, ..., 386, 696, 100\\], \\[ 0, 0, 0, ..., 82, 259,\n",
        "> 2\\], \\[ 0, 0, 0, ..., 296, 27, 338\\], ...,  \n",
        "> \\[ 0, 0, 0, ..., 621, 377, 190\\], \\[ 0, 0, 0, ..., 93, 143, 11\\], \\[\n",
        "> 0, 0, 0, ..., 408, 744, 480\\]\\])\n",
        ">\n",
        "> accr = model.evaluate(test_sequences_matrix,Y_test)\n",
        "> print('Accuracy:',accr\\[1\\])  \n",
        "> print('Loss:',accr\\[0\\])\n",
        ">\n",
        "> 27/27 \\[==============================\\] - 0s 13ms/step - loss: 0.0951\n",
        "> accuracy: 0.9749  \n",
        "> Accuracy: 0.9748803973197937  \n",
        "> Loss: 0.09510093927383423"
      ],
      "id": "MKAyR-KillqR"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": []
    }
  }
}